{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hijera/NN_colab_hw/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D0%B5%D0%B5_%D0%B7%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BA_%D0%B7%D0%B0%D0%BD%D1%8F%D1%82%D0%B8%D1%8E_%C2%AB%D0%9A%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F_%D0%B2_%D0%90%D0%9E%D0%A2%C2%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcadf352",
      "metadata": {
        "id": "dcadf352"
      },
      "source": [
        "## Сделать классификацию данных fakenews\n",
        "1. Используя ноутбук занятия (также размещен в папке Materials) и данные fakenews, 3 раза разными способами получить на задаче классификации значение f1 выше 0.91 для методов на sklearn и выше 0.52 для методов на pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414d77c8",
      "metadata": {
        "id": "414d77c8",
        "outputId": "730f19b7-4189-469f-a808-6aa078d6fe3d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>States reported 1121 deaths a small rise from ...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Populous states can generate large case counts...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                              tweet label\n",
              "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
              "1   2  States reported 1121 deaths a small rise from ...  real\n",
              "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
              "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
              "4   5  Populous states can generate large case counts...  real"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Constraint_Train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71ce694",
      "metadata": {
        "id": "a71ce694",
        "outputId": "b2c3e0db-81c6-4e02-dfcd-9ba57cbc6540"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
              "      <td>real</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>States reported 1121 deaths a small rise from ...</td>\n",
              "      <td>real</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
              "      <td>fake</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
              "      <td>real</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Populous states can generate large case counts...</td>\n",
              "      <td>real</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                              tweet label  label_c\n",
              "0   1  The CDC currently reports 99031 deaths. In gen...  real        1\n",
              "1   2  States reported 1121 deaths a small rise from ...  real        1\n",
              "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake        0\n",
              "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real        1\n",
              "4   5  Populous states can generate large case counts...  real        1"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Преобразуем label в 2 класса, 0 если fake, 1 если real\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "df['label_c']= label_encoder.fit_transform(df['label'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0cc1f4",
      "metadata": {
        "id": "0b0cc1f4"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b7cb78",
      "metadata": {
        "id": "11b7cb78",
        "outputId": "e91edad5-c392-4796-a024-a3d62637a0d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Антон\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7374b8c1",
      "metadata": {
        "id": "7374b8c1",
        "outputId": "11e1e458-b746-4be3-f3e7-13ad3a3084f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 6744.36it/s]\n"
          ]
        }
      ],
      "source": [
        "sentences = [word_tokenize(text.lower()) for text in tqdm(df.tweet)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c7ce9f",
      "metadata": {
        "id": "e4c7ce9f",
        "outputId": "c21368ef-c6aa-4210-9680-1d8d0fb0ee4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 4.52 s\n",
            "Wall time: 1.42 s\n"
          ]
        }
      ],
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "%time model_tweets = Word2Vec(sentences, workers=4,  min_count=3, window=5,vector_size=300,epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a71f49",
      "metadata": {
        "id": "a7a71f49",
        "outputId": "3497117c-3ec5-4740-dbd7-21c86a6de184"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Антон\\AppData\\Local\\Temp\\ipykernel_17652\\2538727259.py:2: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  model_tweets.init_sims()\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "model_tweets.init_sims()\n",
        "def get_text_embedding(text,fsize=300):\n",
        "    result = []\n",
        "    for word in word_tokenize(text.lower()):\n",
        "        if word in model_tweets.wv:\n",
        "            result.append(model_tweets.wv[word])\n",
        "\n",
        "    if len(result):\n",
        "        result = np.sum(result, axis=0)\n",
        "    else:\n",
        "        result = np.zeros(fsize)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d66a781",
      "metadata": {
        "id": "2d66a781",
        "outputId": "8d22b00d-d520-471f-e4c9-d032a45cc635"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:01<00:00, 4862.54it/s]\n"
          ]
        }
      ],
      "source": [
        "features = [get_text_embedding(text) for text in tqdm(df.tweet)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d0e838",
      "metadata": {
        "id": "04d0e838",
        "outputId": "27356f53-d785-4c53-f8ec-d9553b79986f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 4.13011789e+00,  1.02443466e+01, -5.47356987e+00, -9.64809418e+00,\n",
              "       -1.44260550e+00, -5.19524956e+00, -1.28986359e+00,  1.22126780e+01,\n",
              "       -1.21712422e+00, -1.77339983e+00,  7.14585257e+00, -1.93240309e+00,\n",
              "       -3.40784740e+00, -1.03475559e+00,  3.25792503e+00,  1.28895450e+00,\n",
              "        1.27556229e+01,  1.51693165e+00, -3.43241304e-01,  7.22369909e+00,\n",
              "       -7.79073048e+00,  4.46420765e+00,  1.10555897e+01,  8.70104790e+00,\n",
              "        5.62481451e+00,  1.02765446e+01, -5.02773428e+00,  6.04070807e+00,\n",
              "        2.94398141e+00, -4.71269894e+00,  9.28076935e+00, -9.04053450e-01,\n",
              "       -8.25742531e+00, -9.09383416e-01, -1.56467155e-01, -3.40947032e-01,\n",
              "       -2.85702497e-01, -5.70674610e+00,  6.38934851e+00, -3.92306709e+00,\n",
              "       -6.47618484e+00, -3.44814992e+00,  3.74931669e+00, -6.22497678e-01,\n",
              "       -6.53364563e+00,  4.91021061e+00,  2.38904619e+00,  9.45340037e-01,\n",
              "       -9.44050789e+00,  7.70023632e+00, -3.73809934e+00,  2.19432139e+00,\n",
              "       -6.44470358e+00, -1.19723186e+01,  3.89351392e+00,  2.17389655e+00,\n",
              "        6.97655439e+00,  8.63956928e+00,  2.04862285e+00, -2.98000622e+00,\n",
              "       -5.32467699e+00, -2.37841797e+00,  4.66105080e+00,  1.41697288e+00,\n",
              "       -1.41827989e+00,  7.36468601e+00,  8.50152588e+00, -3.91578937e+00,\n",
              "       -4.11549950e+00,  2.08234525e+00,  5.90470457e+00,  8.44550788e-01,\n",
              "       -2.92567039e+00, -3.02254915e+00, -4.83765793e+00,  8.75050545e+00,\n",
              "       -5.84750366e+00, -5.16104984e+00, -7.06380272e+00,  3.68036938e+00,\n",
              "        3.10146546e+00,  7.43871975e+00,  1.49373064e+01,  1.43371449e+01,\n",
              "        5.19283199e+00,  5.94968557e+00,  1.10982388e-01, -4.32017773e-01,\n",
              "       -3.21719718e+00,  1.14359498e-01,  6.59296465e+00, -4.12118435e+00,\n",
              "       -8.82069468e-01, -5.68562627e-01, -4.69633675e+00,  2.37069535e+00,\n",
              "        7.05473709e+00, -3.63747168e+00, -3.85485649e+00,  6.77455616e+00,\n",
              "       -5.25194407e+00,  4.47913313e+00, -5.72198677e+00, -1.61397147e+00,\n",
              "        3.10942173e+00, -1.69222271e+00,  8.30926800e+00,  1.31136918e+00,\n",
              "       -2.04259753e+00,  7.81856179e-02, -5.15827322e+00,  7.14577961e+00,\n",
              "       -1.19616413e+01, -1.92966592e+00,  1.11706126e+00,  1.17694120e+01,\n",
              "        4.57249308e+00,  2.05313993e+00,  2.21878737e-01, -1.18798132e+01,\n",
              "       -1.19294894e+00,  6.25999022e+00,  6.47073746e-01,  7.30668211e+00,\n",
              "        3.08658361e+00, -1.57716346e+00, -7.58298063e+00, -4.69416332e+00,\n",
              "        3.97216892e+00, -1.35378075e+00,  4.65600920e+00,  8.74440384e+00,\n",
              "       -1.45373762e+00,  2.41835046e+00, -5.07414293e+00,  5.53076267e-01,\n",
              "       -3.45196456e-01, -2.93714666e+00,  4.93669081e+00, -1.43807161e+00,\n",
              "       -2.32477617e+00, -3.59764266e+00, -7.03678036e+00,  3.25740242e+00,\n",
              "        1.16257739e+00, -7.45934916e+00,  1.87686217e+00, -2.71381187e+00,\n",
              "        4.46629715e+00, -7.11758018e-01,  5.61081839e+00, -6.36747646e+00,\n",
              "       -2.65684628e+00, -1.66537309e+00,  2.99103475e+00,  5.58018684e+00,\n",
              "        5.06256676e+00, -8.26353168e+00,  3.61952066e-01,  1.13899684e+00,\n",
              "       -5.10618067e+00, -4.97372657e-01, -1.41244364e+00,  1.22533917e+00,\n",
              "       -5.31779099e+00, -6.78485453e-01,  5.29936075e+00,  7.79589033e+00,\n",
              "       -1.29313302e+00,  7.35579872e+00, -2.23841643e+00, -1.92207670e+00,\n",
              "       -2.41321278e+00, -1.23721969e+00, -1.43138275e+01, -1.82483423e+00,\n",
              "       -4.25368547e+00, -1.77244723e+00, -1.13374281e+01,  1.86483407e+00,\n",
              "       -7.73140907e-01,  6.50184298e+00,  6.61203241e+00,  1.01306009e+01,\n",
              "        5.14721823e+00,  3.57590294e+00,  1.25937757e+01,  7.60821915e+00,\n",
              "        5.01276851e-01, -6.74327993e+00, -3.47086906e-01,  2.40629864e+00,\n",
              "       -9.66697931e-03,  1.97567451e+00, -9.53134060e-01, -2.53779483e+00,\n",
              "        6.42261505e+00, -4.56500721e+00,  4.11831558e-01,  9.20030785e+00,\n",
              "       -1.40697803e+01,  5.98116875e+00,  3.32413936e+00, -1.76563597e+00,\n",
              "       -6.82785702e+00,  1.13492012e+00, -1.85479212e+00, -3.43732619e+00,\n",
              "       -6.74182177e+00,  7.28051853e+00, -3.24836731e-01, -5.19776535e+00,\n",
              "       -2.63035798e+00,  1.01326871e+00, -9.60257149e+00, -2.86709857e+00,\n",
              "        6.04542065e+00, -1.24353619e+01, -8.32573128e+00, -5.10157776e+00,\n",
              "       -4.04484808e-01,  4.26651120e-01, -9.87057388e-01,  5.33465672e+00,\n",
              "        2.49704814e+00, -2.22104773e-01, -3.13001800e+00, -3.23058188e-01,\n",
              "       -7.74638236e-01,  2.18983591e-02, -5.45610487e-01,  6.40384579e+00,\n",
              "       -7.47101593e+00, -3.74993253e+00, -2.43445730e+00, -6.07157469e-01,\n",
              "        5.69597840e-01, -2.80678034e+00,  1.34933865e+00, -1.48760772e+00,\n",
              "        3.22034740e+00, -3.96976733e+00, -1.81402302e+00,  2.60620594e-01,\n",
              "       -3.99449849e+00, -3.53458428e+00,  8.13114941e-01,  3.61134982e+00,\n",
              "       -8.07191193e-01,  7.27940702e+00,  2.00436401e+00,  9.22078037e+00,\n",
              "        1.77920568e+00, -1.51033413e+00, -1.84169273e+01,  1.76620865e+00,\n",
              "        2.69490147e+00,  4.51268053e+00, -1.91062808e+00, -9.50138474e+00,\n",
              "        1.09874067e+01,  9.67114568e-01, -4.18093634e+00, -1.32124767e+01,\n",
              "        6.07608128e+00,  6.15106821e+00, -9.76124954e+00,  1.42441916e+00,\n",
              "        8.36087036e+00,  8.40128005e-01,  5.82470417e-01, -6.90376711e+00,\n",
              "        2.20095038e+00,  3.68239093e+00,  1.21966660e+00,  3.28359747e+00,\n",
              "       -2.82175779e-01, -2.84141088e+00,  3.99409199e+00, -3.94024467e+00,\n",
              "       -3.39773965e+00, -4.02513552e+00, -3.36451554e+00, -1.10266304e+00,\n",
              "       -2.31219077e+00, -9.49580479e+00,  2.43682653e-01,  6.16128731e+00,\n",
              "        1.18087101e+00, -3.18773270e-01,  7.79182005e+00,  4.68817329e+00,\n",
              "        1.64848357e-01,  1.62201357e+00,  7.04809964e-01,  7.42460871e+00,\n",
              "       -1.92677796e-01, -1.81608987e+00, -7.80868816e+00,  5.08145094e+00],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b508e6a0",
      "metadata": {
        "id": "b508e6a0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f5be72",
      "metadata": {
        "id": "69f5be72"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fdab90",
      "metadata": {
        "id": "80fdab90"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, df.label_c, test_size=0.33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf28c01b",
      "metadata": {
        "id": "bf28c01b",
        "outputId": "6f90a566-c901-443c-9707-bc4cfde05db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92      1034\n",
            "           1       0.92      0.94      0.93      1085\n",
            "\n",
            "    accuracy                           0.92      2119\n",
            "   macro avg       0.93      0.92      0.92      2119\n",
            "weighted avg       0.93      0.92      0.92      2119\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Используем SVC\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "predicted = model.predict(X_test)\n",
        "print(classification_report(y_test, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0662a634",
      "metadata": {
        "id": "0662a634",
        "outputId": "54022173-8320-4250-85c0-01e0553d77f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92      1034\n",
            "           1       0.92      0.94      0.93      1085\n",
            "\n",
            "    accuracy                           0.92      2119\n",
            "   macro avg       0.92      0.92      0.92      2119\n",
            "weighted avg       0.92      0.92      0.92      2119\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Используем AdaBoost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model = AdaBoostClassifier(n_estimators=256, algorithm=\"SAMME\",)\n",
        "model.fit(X_train, y_train)\n",
        "predicted = model.predict(X_test)\n",
        "print(classification_report(y_test, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279a0bc7",
      "metadata": {
        "id": "279a0bc7",
        "outputId": "60a900a3-af60-4353-dbc0-11bf5abb7da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92      1034\n",
            "           1       0.92      0.93      0.92      1085\n",
            "\n",
            "    accuracy                           0.92      2119\n",
            "   macro avg       0.92      0.92      0.92      2119\n",
            "weighted avg       0.92      0.92      0.92      2119\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#KNeghbours\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=11)\n",
        "model.fit(X_train, y_train)\n",
        "predicted = model.predict(X_test)\n",
        "print(classification_report(y_test, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c9507f",
      "metadata": {
        "id": "12c9507f"
      },
      "outputs": [],
      "source": [
        "netsize=150\n",
        "embsize=400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f8fa05",
      "metadata": {
        "id": "23f8fa05"
      },
      "outputs": [],
      "source": [
        "#Попробуем собрать нейросеть на pytorch\n",
        "def get_word_embedding(tokens, max_len=200,tweets=model_tweets,fsize=300):\n",
        "    result = []\n",
        "    for i in range(max_len):\n",
        "        if i < len(tokens):\n",
        "            word = tokens[i]\n",
        "            if word in tweets.wv:\n",
        "                result.append(tweets.wv[word])\n",
        "            else:\n",
        "                result.append(np.zeros(fsize))\n",
        "        else:\n",
        "            result.append(np.zeros(fsize))\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d212d23d",
      "metadata": {
        "id": "d212d23d",
        "outputId": "891016ad-1d31-4013-e50f-96032eb6a3e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:01<00:00, 3601.51it/s]\n"
          ]
        }
      ],
      "source": [
        "model_tweets_pt = Word2Vec(sentences, workers=4,  min_count=3, window=5,vector_size=netsize,epochs=20)\n",
        "token_lists = [word_tokenize(text.lower()) for text in df.tweet]\n",
        "features = [get_word_embedding(text, embsize,model_tweets_pt,netsize) for text in tqdm(token_lists)]\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, df.label_c, test_size=0.33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f9e11d",
      "metadata": {
        "id": "47f9e11d",
        "outputId": "8b1ae9e1-1d60-4d2b-aa65-7103a9669a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4301, 400, 150)\n",
            "(4301,)\n"
          ]
        }
      ],
      "source": [
        "print(np.array(X_train).shape)\n",
        "print(np.array(y_train).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da206120",
      "metadata": {
        "id": "da206120"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "        self.lstm= nn.LSTM(netsize,150)\n",
        "        self.linear1 = nn.Linear(netsize,75) #\n",
        "        self.relu1 = nn.ReLU() #\n",
        "        self.linear2 = nn.Linear(75,50)#\n",
        "        self.relu2 = nn.ReLU() #\n",
        "        self.linear3 = nn.Linear(50,16)\n",
        "        self.relu3 = nn.ReLU() #\n",
        "\n",
        "        self.linear4 = nn.Linear(16,1) # IN 20 OUT 2 | OUTPUT\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        embeddings, (shortterm, longterm) = self.lstm(x.transpose(0, 1))\n",
        "        out = self.linear1(longterm) # Input Layer\n",
        " #       out= self.linear1(x)\n",
        "        #out = self.relu1(out)\n",
        "\n",
        "        out = self.linear2(out) # Hidden Layer\n",
        "        #out = self.relu2(out)\n",
        "        out = self.linear3(out) # Hidden Layer\n",
        "        #out = self.relu3(out)\n",
        "        out = self.linear4(out) # Hidden Layer\n",
        "\n",
        "        prediction = torch.sigmoid(out)\n",
        "        return prediction\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b12a787",
      "metadata": {
        "id": "2b12a787"
      },
      "outputs": [],
      "source": [
        "in_data = torch.tensor(X_train).float()\n",
        "targets = torch.tensor(np.array(y_train)).float() #.type(torch.LongTensor)\n",
        "optimizer = optim.AdamW(net.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a499b978",
      "metadata": {
        "id": "a499b978"
      },
      "outputs": [],
      "source": [
        "in_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4086711",
      "metadata": {
        "id": "c4086711"
      },
      "outputs": [],
      "source": [
        "targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae77952",
      "metadata": {
        "id": "3ae77952"
      },
      "outputs": [],
      "source": [
        "#X_train = torch.Tensor(X_train)\n",
        "\n",
        "# You must convert it into LongTensor. I did it once\n",
        "#y_train = torch.Tensor(y_train).type(torch.LongTensor)\n",
        "\n",
        "#X_test = torch.Tensor(X_test)\n",
        "#y_test = torch.Tensor(y_test)\n",
        "#error = nn.CrossEntropyLoss()\n",
        "#EPOCHS = 20\n",
        "\n",
        "#for epoch in range(EPOCHS):\n",
        "\n",
        "    # Clearing gradients\n",
        "#    optimizer.zero_grad()\n",
        "\n",
        "    # Forward Propagation\n",
        "#    outs = net(X_train)\n",
        "\n",
        "    # Computing Loss\n",
        "#    loss = criterion(outs,y_train.unsqueeze(-1))\n",
        "\n",
        "    # Backward propagation\n",
        "#    loss.backward()\n",
        "\n",
        "    # Updating parameters\n",
        "#    optimizer.step()\n",
        "\n",
        "    # Printing loss\n",
        "#    print(f\"Loss after iteration {epoch} is {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5a3cf6",
      "metadata": {
        "id": "cf5a3cf6"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(in_data, targets, batch_size=8):\n",
        "    for i in tqdm(range(0, in_data.shape[0], batch_size)):\n",
        "        batch_x = in_data[i:i + batch_size]\n",
        "        batch_y = targets[i:i + batch_size]\n",
        "        #print(batch_x.shape)\n",
        "        #print(batch_y.shape)\n",
        "        optimizer.zero_grad()\n",
        "        output = net(batch_x)\n",
        "        loss = criterion(output.reshape(-1), batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702f6b0f",
      "metadata": {
        "id": "702f6b0f",
        "outputId": "fd8a3292-04af-4ef8-b110-11d6bb0fbe0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 538/538 [05:41<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6617, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 538/538 [05:40<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6625, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 538/538 [05:40<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6627, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 538/538 [05:41<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6627, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 538/538 [05:40<00:00,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6628, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    train_one_epoch(in_data, targets,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3b799b",
      "metadata": {
        "id": "3c3b799b",
        "outputId": "b2f82117-9dff-4542-95b1-c83a363b62ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5176970268994809"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "in_data_test = torch.tensor(X_test).float()\n",
        "targets_test = torch.tensor(np.array(y_test)).float()\n",
        "with torch.no_grad():\n",
        "    output = net(in_data_test).reshape(-1)\n",
        "result = (output > 0.5) == targets_test\n",
        "result.sum().item() / len(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb05ba3",
      "metadata": {
        "id": "5bb05ba3",
        "outputId": "0abc25c3-a949-4c93-cc3d-6890e1d60d6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████| 68/68 [05:12<00:00,  4.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.7061, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████| 68/68 [05:13<00:00,  4.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.7061, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Еще чуть чтуь покрутим?\n",
        "for i in range(2):\n",
        "    train_one_epoch(in_data, targets,64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b37d78",
      "metadata": {
        "id": "71b37d78",
        "outputId": "7f4d4664-2ca9-48d7-859e-6e9bc8a45f79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5176970268994809"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "in_data_test = torch.tensor(X_test).float()\n",
        "targets_test = torch.tensor(np.array(y_test)).float()\n",
        "with torch.no_grad():\n",
        "    output = net(in_data_test).reshape(-1)\n",
        "result = (output > 0.5) == targets_test\n",
        "result.sum().item() / len(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cef2ae20",
      "metadata": {
        "id": "cef2ae20"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}